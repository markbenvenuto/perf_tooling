{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "#pip install pandas matplotlib scipy numpy seaborn natsort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import pandas\n",
    "import warnings\n",
    "import requests\n",
    "import subprocess\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from scipy import stats\n",
    "import json\n",
    "import os\n",
    "import sys\n",
    "\n",
    "sys.path.insert(0, \"../src\")\n",
    "from perf_tools.analysis import make_differential_frame, get_data, get_summary_statistics\n",
    "from perf_tools.analysis import check_are_close, make_latency_plot, plot_latency_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VARIANTS = {\"replset\": \"linux-1-node-replSet-fle.2022-11\", \"sharded\": \"linux-shard-lite-qebench\"}\n",
    "WORKDIR=\"../datasets/genny2/perf1\"\n",
    "\n",
    "patch_id = \"639aa0d661837d305582a2f7\"\n",
    "exec_idx=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "EXPERIMENTS = [\n",
    "  {\n",
    "    # Experiment Set q.1: Query unencrypted fields on unencrypted collection\n",
    "    \"name\" : \"es1\",\n",
    "    \"coll\" : \"pbl\",\n",
    "    \"encryptedFieldCount\" : 0,\n",
    "    \"threadCounts\" : [1,4,8,16],\n",
    "    #\"contentionFactors\" : [1,4,8,16],\n",
    "    \"contentionFactors\" : [1],\n",
    "    \"queries\" : [\n",
    "      {\n",
    "        \"field\" : \"fixed_10\",\n",
    "        \"value\" : \"fixed_hf\"\n",
    "      },\n",
    "      {\n",
    "        \"field\" : \"fixed_10\",\n",
    "        \"value\" : \"uar\"\n",
    "      },\n",
    "      {\n",
    "        \"field\" : \"uar_[1,10]\",\n",
    "        \"value\" : \"uar\"\n",
    "      },\n",
    "      {\n",
    "        \"field\" : \"uar_[1,10]\",\n",
    "        \"value\" : \"uar_alllow\"\n",
    "      },\n",
    "    ]\n",
    "  },\n",
    "  {\n",
    "    # Experiment Set q.2: Query unencrypted fields on partially encrypted collection\n",
    "    \"name\" : \"es2\",\n",
    "    \"coll\" : \"pbl\",\n",
    "    \"encryptedFieldCount\" : 5,\n",
    "    \"threadCounts\" : [1,4,8,16],\n",
    "    \"contentionFactors\" : [1,4,8,16],\n",
    "    \"queries\" : [\n",
    "      {\n",
    "        \"field\" : \"fixed_10\",\n",
    "        \"value\" : \"fixed_hf\"\n",
    "      },\n",
    "      {\n",
    "        \"field\" : \"fixed_10\",\n",
    "        \"value\" : \"uar\"\n",
    "      },\n",
    "      {\n",
    "        \"field\" : \"uar_[6,10]\",\n",
    "        \"value\" : \"uar\"\n",
    "      },\n",
    "      {\n",
    "        \"field\" : \"uar_[6,10]\",\n",
    "        \"value\" : \"uar_alllow\"\n",
    "      },\n",
    "    ]\n",
    "  },\n",
    "  {\n",
    "    # Experiment Set q.3: Query encrypted fields on partially encrypted collection\n",
    "    \"name\" : \"es3\",\n",
    "    \"coll\" : \"pbl\",\n",
    "    \"encryptedFieldCount\" : 5,\n",
    "    \"threadCounts\" : [1,4,8,16],\n",
    "    \"contentionFactors\" : [1,4,8,16],\n",
    "    \"queries\" : [\n",
    "      {\n",
    "        \"field\" : \"fixed_1\",\n",
    "        \"value\" : \"fixed_hf\"\n",
    "      },\n",
    "      {\n",
    "        \"field\" : \"fixed_1\",\n",
    "        \"value\" : \"uar\"\n",
    "      },\n",
    "      {\n",
    "        \"field\" : \"uar_[1,5]\",\n",
    "        \"value\" : \"uar\"\n",
    "      },\n",
    "      {\n",
    "        \"field\" : \"uar_[1,5]\",\n",
    "        \"value\" : \"uar_alllow\"\n",
    "      },\n",
    "    ]\n",
    "  },\n",
    "  {\n",
    "    # Experiment Set q.4: Query encrypted fields on fully encrypted collection\n",
    "    \"name\" : \"es4\",\n",
    "    \"coll\" : \"pbl\",\n",
    "    \"encryptedFieldCount\" : 10,\n",
    "    \"threadCounts\" : [1,4,8,16],\n",
    "    \"contentionFactors\" : [1,4,8,16],\n",
    "    \"queries\" : [\n",
    "      {\n",
    "        \"field\" : \"fixed_1\",\n",
    "        \"value\" : \"fixed_hf\"\n",
    "      },\n",
    "      {\n",
    "        \"field\" : \"fixed_1\",\n",
    "        \"value\" : \"uar\"\n",
    "      },\n",
    "      {\n",
    "        \"field\" : \"uar_[1,10]\",\n",
    "        \"value\" : \"uar\"\n",
    "      },\n",
    "      {\n",
    "        \"field\" : \"uar_[1,10]\",\n",
    "        \"value\" : \"uar_alllow\"\n",
    "      },\n",
    "    ]\n",
    "  },\n",
    "  # {\n",
    "  #   # Experiment Set q.5: Check the impact of BSON limit on queries on both encrypted and unencrypted fields\n",
    "  #   \"name\" : \"es5\",\n",
    "  #   \"coll\" : \"blimit\",\n",
    "  #   \"encryptedFieldCount\" : 5,\n",
    "  #   \"threadCounts\" : [1,4,8,16],\n",
    "  #   \"contentionFactors\" : [1,4,8,16],\n",
    "  #   \"queries\" : [\n",
    "  #     {\n",
    "  #       \"field\" : \"fixed_1\",\n",
    "  #       \"value\" : \"fixed_hf\"\n",
    "  #     },\n",
    "  #     {\n",
    "  #       \"field\" : \"fixed_10\",\n",
    "  #       \"value\" : \"fixed_hf\"\n",
    "  #     },\n",
    "  #   ]\n",
    "  # },\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataSetCache:\n",
    "    def __init__(self, workdir, patch_id, variant, execution, task_name, experiment, cf, tc, query_num, query, phase_name):\n",
    "        self.workdir = workdir\n",
    "        self.patch_id = patch_id\n",
    "        self.variant = variant\n",
    "        self.execution = execution\n",
    "        self.task_name = task_name\n",
    "        self.experiment = experiment\n",
    "        self.cf = cf\n",
    "        self.tc = tc\n",
    "        self.query_num = query_num\n",
    "        self.phase_name = phase_name\n",
    "        self.variant = variant\n",
    "\n",
    "        self.query = query\n",
    "        self.data = None\n",
    "\n",
    "    def json_path(self, metric):\n",
    "        return os.path.join(self.workdir, self.patch_id, self.variant,\n",
    "            self.task_name, str(self.execution), metric + \".json\")\n",
    "\n",
    "    def get_data(self):\n",
    "        if self.data is None:\n",
    "            self.data = get_data(self.json_path(\"InsertActor.\" + self.phase_name))\n",
    "        return self.data\n",
    "\n",
    "DATASETS = {}\n",
    "\n",
    "for ex in EXPERIMENTS:\n",
    "    for cf in ex[\"contentionFactors\"]:\n",
    "        for tc in ex[\"threadCounts\"]:\n",
    "            experiment = ex['name']\n",
    "            testName = f\"query_{ex['name']}_{cf}_{tc}\"\n",
    "            fullName = testName + \".load\"\n",
    "\n",
    "            DATASETS[fullName] = DataSetCache(WORKDIR, patch_id, VARIANTS[\"replset\"], \"0\", testName, experiment, cf, tc, \"load\", \"load\", \"load.inserts\")\n",
    "\n",
    "            for i, query in enumerate(ex[\"queries\"]):\n",
    "                qi = i + 1\n",
    "                fullName = testName + f\".q{qi}.reads\"\n",
    "\n",
    "                DATASETS[fullName] = DataSetCache(WORKDIR, patch_id, VARIANTS[\"replset\"], \"0\", testName, experiment, cf, tc, qi, f\"{query['field']}: {query['value']}\", f\"q{qi}.reads\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_load_datacache(experiment_name, cf, tc):\n",
    "    ds_name = f\"query_{experiment_name}_{cf}_{tc}.load\"\n",
    "    return DATASETS[ds_name]\n",
    "\n",
    "def get_query_datacache(experiment_name, cf, tc, query_number):\n",
    "    ds_name = f\"query_{experiment_name}_{cf}_{tc}.q{query_number}.reads\"\n",
    "    return DATASETS[ds_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get summary statistics for all data sets\n",
    "# WARNING: may take several minutes\n",
    "def get_summary_statistics_df(ds_cache):\n",
    "    l1 = ds_cache.get_data()\n",
    "    b = l1.diff_data\n",
    "    fixed_data = l1.fixed_data\n",
    "    raw_data = l1.raw_data\n",
    "\n",
    "    quantiles = stats.mstats.mquantiles(b.loc[:,\"pure_latency\"].values, prob=[0.5,0.8,0.9,0.95,0.99], alphap=1/3, betap=1/3)\n",
    "    averages = b.mean(numeric_only=True)\n",
    "    maximum = b.max()\n",
    "    minimum = b.min()\n",
    "    duration = (b[\"ts\"][len(b)-1] - b[\"ts\"][0]).total_seconds()\n",
    "    ops = fixed_data[\"d(ops)\"].sum()\n",
    "    size = fixed_data[\"d(size)\"].sum()\n",
    "    docs = fixed_data[\"d(n)\"].sum()\n",
    "    errs = fixed_data[\"d(err)\"].sum()\n",
    "    overhead = fixed_data[\"d(t_overhead)\"].sum()\n",
    "\n",
    "    df = pandas.DataFrame(index = [0], data = {\n",
    "        'Task' : ds_cache.task_name + \".\" + ds_cache.phase_name,\n",
    "        'TaskName' : ds_cache.task_name,\n",
    "        'Experiment' : ds_cache.experiment,\n",
    "        'Phase' : ds_cache.phase_name,\n",
    "        'Query' : ds_cache.query,\n",
    "        'QueryNumber' : ds_cache.query_num,\n",
    "        'ContentionFactor': ds_cache.cf,\n",
    "        'ThreadCount': ds_cache.tc,\n",
    "        'AverageLatency': averages[\"pure_latency\"],\n",
    "        'AverageLatencyMillis': averages[\"pure_latency(ms)\"],\n",
    "        'AverageSize': size / ops,\n",
    "        'OperationThroughput': ops / duration,\n",
    "        'DocumentThroughput': docs / duration,\n",
    "        'SizeThroughput': size / duration,\n",
    "        'ErrorRate': errs / duration,\n",
    "        'Latency50thPercentile': quantiles[0],\n",
    "        'Latency80thPercentile': quantiles[1],\n",
    "        'Latency90thPercentile': quantiles[2],\n",
    "        'Latency95thPercentile': quantiles[3],\n",
    "        'Latency99thPercentile': quantiles[4],\n",
    "        'WorkersMin': raw_data[\"gauges.workers\"].min(),\n",
    "        'WorkersMax': raw_data[\"gauges.workers\"].max(),\n",
    "        'LatencyMax': maximum[\"pure_latency\"],\n",
    "        'LatencyMin': minimum[\"pure_latency\"],\n",
    "        'DurationTotal': duration * 1e9,\n",
    "        'ErrorsTotal': errs,\n",
    "        'OperationsTotal': ops,\n",
    "        'DocumentsTotal': docs,\n",
    "        'SizeTotal': size,\n",
    "        'OverheadTotal': overhead\n",
    "    })\n",
    "\n",
    "    return df\n",
    "\n",
    "df = None\n",
    "\n",
    "for key in DATASETS.keys():\n",
    "    if df is None:\n",
    "        df = get_summary_statistics_df(DATASETS[key])\n",
    "    else:\n",
    "        df = pandas.concat([df, get_summary_statistics_df(DATASETS[key])], ignore_index=True)\n",
    "\n",
    "df.to_csv(\"/tmp/summary.csv\")\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graph latency for load for a given experiment\n",
    "#\n",
    "experiment = \"es2\"\n",
    "cf = 1\n",
    "tc = 8\n",
    "e_datacache = get_load_datacache(experiment, cf, tc)\n",
    "df_w = e_datacache.get_data().diff_data\n",
    "\n",
    "# Graph scatter plot of latency\n",
    "df_w.plot(figsize=(20,20), x=\"total_ops\", y= \"pure_latency(ms)\", kind=\"scatter\", title=f\"Load latency: {experiment} cf_{cf} thread_{tc}\", ylabel=\"pure_latency(ms)\", grid=0.4)\n",
    "\n",
    "# Graph latency with trend lines\n",
    "plot_latency_stats(df_w, \"total_ops\", title=f\"{experiment} cf_{cf} thread_{tc} insert stats\", regr=\"log\", start=start, end=end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graph latency for query for a given experiment\n",
    "#\n",
    "experiment = \"es2\"\n",
    "cf = 1\n",
    "tc = 8\n",
    "e_datacache = get_query_datacache(experiment, cf, tc, 1)\n",
    "df_w = e_datacache.get_data().diff_data\n",
    "\n",
    "# Graph scatter plot of latency\n",
    "df_w.plot(figsize=(20,20), x=\"total_ops\", y= \"pure_latency(ms)\", kind=\"scatter\", title=f\"Load latency: {experiment} cf_{cf} thread_{tc}\", ylabel=\"pure_latency(ms)\", grid=0.4)\n",
    "\n",
    "# Graph latency with trend lines\n",
    "plot_latency_stats(df_w, \"total_ops\", title=f\"{experiment} cf_{cf} thread_{tc} insert stats\", regr=\"log\", start=start, end=end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graph throughput for given load experiment pivoted on thread count\n",
    "experiment = \"es1\"\n",
    "cf = 1\n",
    "threadCounts = [1,4,8,16]\n",
    "columns = []\n",
    "column_data = []\n",
    "\n",
    "for tc in threadCounts:\n",
    "    \n",
    "   \n",
    "    e_datacache = get_load_datacache(experiment, cf, tc)\n",
    "    df_w = e_datacache.get_data().diff_data\n",
    "    \n",
    "    name = f\"cf_{cf}_thread_{tc}_throughput\"\n",
    "    column_data.append(df_w[\"throughput\"])\n",
    "    columns.append(name)\n",
    "    print(name)\n",
    "        \n",
    "df = pandas.concat(column_data, axis=\"columns\")\n",
    "df.columns =columns\n",
    "df[100:].plot(figsize=(20,20), title=f\"Load throughput {experiment}\", ylabel=\"throughput (ops/sec)\", grid=0.4)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graph throughput for given query experiment pivoted on thread count and contention factor\n",
    "def plot_throughput_set(experiment, query_number, contentionFactors, threadCounts):\n",
    "    columns = []\n",
    "    column_data = []\n",
    "\n",
    "    for cf in contentionFactors:\n",
    "        for tc in threadCounts:\n",
    "            e_datacache = get_query_datacache(experiment, cf, tc, query_number)\n",
    "            df_w = e_datacache.get_data().diff_data\n",
    "            \n",
    "            name = f\"cf_{cf}_thread_{tc}_throughput\"\n",
    "            column_data.append(df_w[\"throughput\"])\n",
    "            columns.append(name)\n",
    "                \n",
    "    df = pandas.concat(column_data, axis=\"columns\")\n",
    "    df.columns =columns\n",
    "    df[100:].plot(figsize=(20,20), title=f\"Read throughput {experiment} Query {query_number}\", ylabel=\"throughput (ops/sec)\", grid=0.4)\n",
    "\n",
    "plot_throughput_set(\"es2\", 1, [1,4,8,16], [1,4,8,16])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment = \"query_es2\"\n",
    "contentionFactors = [1]\n",
    "threadCounts = [1,4,8,16]\n",
    "\n",
    "es_workloads = {}\n",
    "for cf in contentionFactors:\n",
    "    \n",
    "    columns = []\n",
    "    column_data = []\n",
    "    for tc in threadCounts:\n",
    "        e_w = ES1Workload(WORKDIR, patch_id, VARIANTS[\"replset\"], \"0\", f\"{experiment}_{cf}_{tc}\")\n",
    "        es_workloads[(cf,tc)] = e_w\n",
    "        df_w = e_w.get_load_data().diff_data\n",
    "        \n",
    "        name = f\"cf_{cf}_thread_{tc}_throughput\"\n",
    "        column_data.append(df_w[\"throughput\"])\n",
    "        columns.append(name)\n",
    "        print(name)\n",
    "        \n",
    "    df = pandas.concat(column_data, axis=\"columns\")\n",
    "    df.columns =columns\n",
    "    print(df.columns)\n",
    "    print(df.info())\n",
    "    print(df.axes)\n",
    "    #df.head()\n",
    "    df.plot(figsize=(20,20), title=\"Load throughput\", ylabel=\"throughput (ops/sec)\", grid=0.4)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
